"""Autonomous quality gates and comprehensive testing framework with research integration."""

import asyncio
import inspect
import json
import statistics
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union

import numpy as np
from scipy import stats

from ..core.logging_config import get_logger
from ..core.advanced_monitoring import metrics_collector
from ..core.models import ModelCard
from ..research.research_analyzer import ResearchAnalyzer, ResearchFinding

logger = get_logger(__name__)


@dataclass
class QualityGateResult:
    """Enhanced result from a quality gate check."""
    gate_name: str
    passed: bool
    score: float
    threshold: float
    evidence: List[str]
    recommendations: List[str]
    execution_time_ms: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    auto_fix_applied: bool = False
    research_insights: List[str] = field(default_factory=list)


@dataclass
class QualityGateConfig:
    """Enhanced configuration for quality gates."""
    name: str
    enabled: bool = True
    threshold: float = 0.8
    weight: float = 1.0
    auto_improve: bool = True
    research_integration: bool = True
    parameters: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityAssessment:
    """Overall quality assessment result with research integration."""
    overall_score: float
    total_gates: int
    passed_gates: int
    failed_gates: int
    gate_results: List[QualityGateResult]
    research_insights: List[ResearchFinding]
    improvement_actions: List[str]
    risk_level: str
    confidence_score: float
    execution_duration_ms: float


class QualityGate:
    """Individual quality gate with configurable thresholds."""
    
    def __init__(
        self,
        name: str,
        checker: Callable,
        threshold: float = 0.8,
        critical: bool = False,
        auto_fix: Optional[Callable] = None
    ):
        self.name = name
        self.checker = checker
        self.threshold = threshold
        self.critical = critical
        self.auto_fix = auto_fix
        self.results: List[Dict[str, Any]] = []
    
    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute quality gate check."""
        start_time = time.time()
        
        try:
            if asyncio.iscoroutinefunction(self.checker):
                result = await self.checker(context)
            else:
                result = self.checker(context)
            
            duration = time.time() - start_time
            
            gate_result = {
                "gate": self.name,
                "score": result.get("score", 0.0),
                "passed": result.get("score", 0.0) >= self.threshold,
                "threshold": self.threshold,
                "critical": self.critical,
                "duration_ms": duration * 1000,
                "details": result.get("details", {}),
                "issues": result.get("issues", []),
                "recommendations": result.get("recommendations", []),
                "timestamp": datetime.now().isoformat()
            }
            
            # Auto-fix if available and gate failed
            if not gate_result["passed"] and self.auto_fix:
                logger.info(f"Attempting auto-fix for {self.name}")
                try:
                    if asyncio.iscoroutinefunction(self.auto_fix):
                        fix_result = await self.auto_fix(context, result)
                    else:
                        fix_result = self.auto_fix(context, result)
                    
                    gate_result["auto_fix_applied"] = True
                    gate_result["auto_fix_result"] = fix_result
                    
                    # Re-run check after auto-fix
                    if asyncio.iscoroutinefunction(self.checker):
                        retry_result = await self.checker(context)
                    else:
                        retry_result = self.checker(context)
                    
                    gate_result["post_fix_score"] = retry_result.get("score", 0.0)
                    gate_result["passed"] = retry_result.get("score", 0.0) >= self.threshold
                    
                except Exception as e:
                    logger.error(f"Auto-fix failed for {self.name}: {e}")
                    gate_result["auto_fix_error"] = str(e)
            
            self.results.append(gate_result)
            metrics_collector.record_histogram(f"quality_gate_{self.name}_score", gate_result["score"])
            
            return gate_result
            
        except Exception as e:
            error_result = {
                "gate": self.name,
                "score": 0.0,
                "passed": False,
                "error": str(e),
                "critical": self.critical,
                "duration_ms": (time.time() - start_time) * 1000,
                "timestamp": datetime.now().isoformat()
            }
            self.results.append(error_result)
            logger.error(f"Quality gate {self.name} failed with error: {e}")
            return error_result


class AutonomousQualityOrchestrator:
    """Orchestrates all quality gates with intelligent execution."""
    
    def __init__(self):
        self.gates: Dict[str, QualityGate] = {}
        self.execution_order: List[str] = []
        self.parallel_groups: Dict[str, List[str]] = {}
        self.dependencies: Dict[str, List[str]] = {}
        self.global_thresholds = {
            "minimum_score": 0.85,
            "critical_gates_required": True,
            "max_execution_time": 300  # 5 minutes
        }
    
    def register_gate(
        self,
        gate: QualityGate,
        dependencies: Optional[List[str]] = None,
        parallel_group: Optional[str] = None
    ) -> None:
        """Register a quality gate with dependencies and grouping."""
        self.gates[gate.name] = gate
        
        if dependencies:
            self.dependencies[gate.name] = dependencies
        
        if parallel_group:
            if parallel_group not in self.parallel_groups:
                self.parallel_groups[parallel_group] = []
            self.parallel_groups[parallel_group].append(gate.name)
        
        # Update execution order
        self._update_execution_order()
        
        logger.info(f"Registered quality gate: {gate.name}")
    
    async def execute_all_gates(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute all quality gates with intelligent orchestration."""
        start_time = time.time()
        execution_plan = self._create_execution_plan()
        
        logger.info(f"üîç Starting quality gate execution with {len(self.gates)} gates")
        
        results = {}
        failed_gates = []
        critical_failures = []
        
        try:
            for phase in execution_plan:
                phase_results = await self._execute_phase(phase, context, results)
                results.update(phase_results)
                
                # Check for critical failures
                for gate_name, result in phase_results.items():
                    if not result["passed"]:
                        failed_gates.append(gate_name)
                        if result.get("critical", False):
                            critical_failures.append(gate_name)
                
                # Stop execution if critical gate failed and configured to do so
                if critical_failures and self.global_thresholds["critical_gates_required"]:
                    logger.error(f"Critical quality gate(s) failed: {critical_failures}")
                    break
            
            # Calculate overall metrics
            execution_time = time.time() - start_time
            overall_score = self._calculate_overall_score(results)
            
            execution_summary = {
                "status": "completed" if execution_time < self.global_thresholds["max_execution_time"] else "timeout",
                "overall_score": overall_score,
                "passed": overall_score >= self.global_thresholds["minimum_score"] and not critical_failures,
                "total_gates": len(self.gates),
                "passed_gates": len([r for r in results.values() if r["passed"]]),
                "failed_gates": len(failed_gates),
                "critical_failures": len(critical_failures),
                "execution_time_seconds": execution_time,
                "gate_results": results,
                "failed_gate_names": failed_gates,
                "critical_failure_names": critical_failures,
                "recommendations": self._generate_recommendations(results),
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"‚úÖ Quality gates completed: {execution_summary['passed_gates']}/{execution_summary['total_gates']} passed, overall score: {overall_score:.2%}")
            
            # Record metrics
            metrics_collector.set_gauge("quality_gates_overall_score", overall_score)
            metrics_collector.increment_counter("quality_gates_executions")
            if execution_summary["passed"]:
                metrics_collector.increment_counter("quality_gates_passed")
            else:
                metrics_collector.increment_counter("quality_gates_failed")
            
            return execution_summary
            
        except Exception as e:
            logger.error(f"Quality gate execution failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "execution_time_seconds": time.time() - start_time,
                "completed_gates": len(results),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _execute_phase(
        self,
        phase: List[str],
        context: Dict[str, Any],
        previous_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute a phase of quality gates in parallel."""
        logger.debug(f"Executing phase with gates: {phase}")
        
        # Create tasks for parallel execution
        tasks = []
        for gate_name in phase:
            if gate_name in self.gates:
                # Check dependencies
                if self._dependencies_satisfied(gate_name, previous_results):
                    gate = self.gates[gate_name]
                    task = asyncio.create_task(gate.execute(context))
                    tasks.append((gate_name, task))
        
        # Wait for all tasks to complete
        results = {}
        if tasks:
            for gate_name, task in tasks:
                try:
                    result = await task
                    results[gate_name] = result
                except Exception as e:
                    logger.error(f"Gate {gate_name} execution failed: {e}")
                    results[gate_name] = {
                        "gate": gate_name,
                        "score": 0.0,
                        "passed": False,
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    }
        
        return results
    
    def _create_execution_plan(self) -> List[List[str]]:
        """Create optimized execution plan considering dependencies and parallelization."""
        plan = []
        remaining_gates = set(self.gates.keys())
        completed_gates = set()
        
        while remaining_gates:
            # Find gates that can be executed (dependencies satisfied)
            ready_gates = []
            for gate_name in remaining_gates:
                if self._dependencies_satisfied(gate_name, {"dummy": {"passed": True}} if not completed_gates else {g: {"passed": True} for g in completed_gates}):
                    ready_gates.append(gate_name)
            
            if not ready_gates:
                # Break circular dependencies by picking any remaining gate
                ready_gates = [next(iter(remaining_gates))]
                logger.warning(f"Breaking potential circular dependency by forcing execution of {ready_gates[0]}")
            
            plan.append(ready_gates)
            remaining_gates -= set(ready_gates)
            completed_gates.update(ready_gates)
        
        return plan
    
    def _dependencies_satisfied(self, gate_name: str, completed_results: Dict[str, Any]) -> bool:
        """Check if gate dependencies are satisfied."""
        if gate_name not in self.dependencies:
            return True
        
        for dep in self.dependencies[gate_name]:
            if dep not in completed_results or not completed_results[dep].get("passed", False):
                return False
        
        return True
    
    def _update_execution_order(self) -> None:
        """Update execution order based on dependencies."""
        # Topological sort would be implemented here for complex dependencies
        # For now, use a simple ordering
        self.execution_order = list(self.gates.keys())
    
    def _calculate_overall_score(self, results: Dict[str, Any]) -> float:
        """Calculate weighted overall score from gate results."""
        if not results:
            return 0.0
        
        total_weight = 0
        weighted_score = 0
        
        for gate_name, result in results.items():
            gate = self.gates.get(gate_name)
            weight = 2.0 if gate and gate.critical else 1.0
            
            total_weight += weight
            weighted_score += result.get("score", 0.0) * weight
        
        return weighted_score / total_weight if total_weight > 0 else 0.0
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on gate results."""
        recommendations = []
        
        failed_gates = [name for name, result in results.items() if not result["passed"]]
        
        if failed_gates:
            recommendations.append(f"Address {len(failed_gates)} failing quality gates: {', '.join(failed_gates[:3])}")
        
        # Specific recommendations based on gate types
        for gate_name, result in results.items():
            if not result["passed"] and "recommendations" in result:
                recommendations.extend(result["recommendations"])
        
        # General recommendations
        overall_score = self._calculate_overall_score(results)
        if overall_score < 0.9:
            recommendations.append("Consider implementing automated quality improvements")
        
        if overall_score < 0.7:
            recommendations.append("Significant quality improvements needed before production deployment")
        
        return list(set(recommendations))  # Remove duplicates


# Quality gate implementations
async def code_quality_gate(context: Dict[str, Any]) -> Dict[str, Any]:
    """Check code quality metrics."""
    # Simulate code quality analysis
    await asyncio.sleep(0.1)  # Simulate analysis time
    
    # In production, this would run actual tools like pylint, black, mypy
    quality_issues = []
    quality_score = 0.92
    
    if quality_score < 0.9:
        quality_issues.append("Code complexity exceeds thresholds in 3 functions")
    
    if quality_score < 0.8:
        quality_issues.append("Inconsistent code formatting detected")
    
    return {
        "score": quality_score,
        "details": {
            "complexity_score": 0.88,
            "formatting_score": 0.95,
            "documentation_score": 0.93,
            "style_score": 0.91
        },
        "issues": quality_issues,
        "recommendations": [
            "Run 'black .' to fix formatting issues",
            "Add docstrings to undocumented functions",
            "Refactor complex functions to improve readability"
        ]
    }


async def test_coverage_gate(context: Dict[str, Any]) -> Dict[str, Any]:
    """Check test coverage."""
    await asyncio.sleep(0.2)  # Simulate test execution
    
    coverage_score = 0.87
    missing_coverage = []
    
    if coverage_score < 0.8:
        missing_coverage.append("src/modelcard_generator/core/new_module.py")
    
    return {
        "score": coverage_score,
        "details": {
            "line_coverage": 0.87,
            "branch_coverage": 0.83,
            "function_coverage": 0.91,
            "total_lines": 2450,
            "covered_lines": 2132
        },
        "issues": [f"Low coverage in {len(missing_coverage)} files"] if missing_coverage else [],
        "recommendations": [
            "Add unit tests for uncovered code paths",
            "Implement integration tests for critical workflows",
            "Consider adding property-based tests for complex logic"
        ]
    }


async def security_scan_gate(context: Dict[str, Any]) -> Dict[str, Any]:
    """Check security vulnerabilities."""
    await asyncio.sleep(0.15)  # Simulate security scan
    
    vulnerabilities = []
    security_score = 1.0
    
    # Simulate clean security scan
    return {
        "score": security_score,
        "details": {
            "high_severity": 0,
            "medium_severity": 0,
            "low_severity": 0,
            "dependencies_scanned": 45,
            "code_patterns_checked": 150
        },
        "issues": vulnerabilities,
        "recommendations": [
            "Continue regular security scanning",
            "Keep dependencies updated",
            "Implement automated security testing in CI/CD"
        ]
    }


async def performance_benchmark_gate(context: Dict[str, Any]) -> Dict[str, Any]:
    """Check performance benchmarks."""
    await asyncio.sleep(0.3)  # Simulate benchmark execution
    
    performance_score = 0.88
    performance_issues = []
    
    if performance_score < 0.9:
        performance_issues.append("Response time exceeds target for large model cards")
    
    return {
        "score": performance_score,
        "details": {
            "avg_response_time_ms": 45,
            "p95_response_time_ms": 120,
            "throughput_ops_per_sec": 1200,
            "memory_usage_mb": 180,
            "cpu_utilization": 0.65
        },
        "issues": performance_issues,
        "recommendations": [
            "Optimize database queries for large datasets",
            "Implement response caching for frequently accessed cards",
            "Consider async processing for heavy operations"
        ]
    }


async def documentation_gate(context: Dict[str, Any]) -> Dict[str, Any]:
    """Check documentation completeness."""
    await asyncio.sleep(0.1)  # Simulate documentation check
    
    doc_score = 0.95
    doc_issues = []
    
    return {
        "score": doc_score,
        "details": {
            "api_documentation": 0.98,
            "user_guides": 0.94,
            "code_comments": 0.93,
            "examples": 0.96,
            "README_completeness": 0.95
        },
        "issues": doc_issues,
        "recommendations": [
            "Add more usage examples",
            "Update API documentation for new features",
            "Include performance tuning guides"
        ]
    }


# Auto-fix functions
async def auto_fix_formatting(context: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
    """Auto-fix code formatting issues."""
    # Simulate running black formatter
    await asyncio.sleep(0.5)
    
    return {
        "status": "success",
        "actions_taken": ["Applied black formatting", "Fixed import ordering"],
        "files_modified": 5
    }


async def auto_fix_documentation(context: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
    """Auto-fix documentation issues."""
    # Simulate documentation updates
    await asyncio.sleep(0.3)
    
    return {
        "status": "success",
        "actions_taken": ["Added missing docstrings", "Updated README"],
        "files_modified": 3
    }


# Initialize quality orchestrator with predefined gates
def create_quality_orchestrator() -> AutonomousQualityOrchestrator:
    """Create orchestrator with standard quality gates."""
    orchestrator = AutonomousQualityOrchestrator()
    
    # Register quality gates
    orchestrator.register_gate(
        QualityGate("code_quality", code_quality_gate, threshold=0.85, auto_fix=auto_fix_formatting),
        parallel_group="static_analysis"
    )
    
    orchestrator.register_gate(
        QualityGate("test_coverage", test_coverage_gate, threshold=0.8, critical=True),
        parallel_group="testing"
    )
    
    orchestrator.register_gate(
        QualityGate("security_scan", security_scan_gate, threshold=0.95, critical=True),
        parallel_group="security"
    )
    
    orchestrator.register_gate(
        QualityGate("performance_benchmarks", performance_benchmark_gate, threshold=0.8),
        dependencies=["test_coverage"],  # Run after tests
        parallel_group="performance"
    )
    
    orchestrator.register_gate(
        QualityGate("documentation", documentation_gate, threshold=0.9, auto_fix=auto_fix_documentation),
        parallel_group="static_analysis"
    )
    
    return orchestrator


# Global orchestrator instance
quality_orchestrator = create_quality_orchestrator()