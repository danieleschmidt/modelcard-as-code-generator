#!/usr/bin/env python3
"""Simple test runner for model card generator."""

import sys
import traceback
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_imports():
    """Test that core modules can be imported."""
    print("Testing imports...")
    
    try:
        from modelcard_generator.core.models import ModelCard, CardConfig, CardFormat
        print("‚úÖ Core models imported successfully")
    except Exception as e:
        print(f"‚ùå Core models import failed: {e}")
        return False
    
    try:
        from modelcard_generator.core.exceptions import ModelCardError
        print("‚úÖ Exceptions imported successfully")
    except Exception as e:
        print(f"‚ùå Exceptions import failed: {e}")
        return False
    
    try:
        from modelcard_generator.core.security import sanitizer, scanner
        print("‚úÖ Security modules imported successfully")
    except Exception as e:
        print(f"‚ùå Security import failed: {e}")
        return False
    
    try:
        from modelcard_generator.core.logging_config import get_logger
        print("‚úÖ Logging imported successfully")
    except Exception as e:
        print(f"‚ùå Logging import failed: {e}")
        return False
    
    try:
        from modelcard_generator.core.config import get_config
        print("‚úÖ Config imported successfully")
    except Exception as e:
        print(f"‚ùå Config import failed: {e}")
        return False
    
    return True

def test_basic_functionality():
    """Test basic model card functionality."""
    print("\nTesting basic functionality...")
    
    try:
        from modelcard_generator.core.models import ModelCard, CardConfig, CardFormat
        
        # Test model card creation
        config = CardConfig(format=CardFormat.HUGGINGFACE)
        card = ModelCard(config=config)
        card.model_details.name = "test-model"
        card.model_details.version = "1.0.0"
        card.add_metric("accuracy", 0.95)
        card.add_limitation("Test limitation")
        
        # Test rendering
        markdown = card.render("markdown")
        assert "test-model" in markdown
        assert "accuracy" in markdown
        print("‚úÖ Basic model card functionality works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Basic functionality test failed: {e}")
        traceback.print_exc()
        return False

def test_security_features():
    """Test security features."""
    print("\nTesting security features...")
    
    try:
        from modelcard_generator.core.security import sanitizer, scan_for_vulnerabilities
        
        # Test input sanitization (should reject malicious content)
        try:
            clean_string = sanitizer.sanitize_string("Test input <script>alert('xss')</script>")
            print("‚ùå Sanitization failed - should have rejected malicious content")
            return False
        except Exception:
            print("‚úÖ Input sanitization correctly rejects malicious content")
        
        # Test with safe content
        clean_string = sanitizer.sanitize_string("This is safe content")
        assert clean_string == "This is safe content"
        print("‚úÖ Input sanitization allows safe content")
        
        # Test security scanning
        test_content = "This is safe content for testing"
        scan_result = scan_for_vulnerabilities(test_content)
        assert "passed" in scan_result
        print("‚úÖ Security scanning works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Security test failed: {e}")
        traceback.print_exc()
        return False

def test_performance_features():
    """Test performance monitoring."""
    print("\nTesting performance features...")
    
    try:
        # Test basic performance structures without system monitoring
        from modelcard_generator.core.performance_monitor import PerformanceMetrics, PerformanceTracker
        
        # Create a basic tracker without system dependencies
        tracker = PerformanceTracker(max_history=10)
        
        # Test operation tracking
        op_id = tracker.start_operation("test_operation")
        import time
        time.sleep(0.01)
        metrics = tracker.end_operation(op_id, "test_operation", success=True)
        
        assert metrics.operation_name == "test_operation"
        assert metrics.success is True
        assert metrics.duration_ms > 0
        print("‚úÖ Performance tracking works")
        
        # Test statistics
        stats = tracker.get_operation_stats("test_operation")
        assert stats["total_calls"] >= 1
        print("‚úÖ Performance statistics work")
        
        return True
        
    except ImportError as e:
        if "psutil" in str(e):
            print("‚ö†Ô∏è  Performance monitoring requires psutil - feature available but system monitoring disabled")
            return True  # This is acceptable
        else:
            print(f"‚ùå Performance test failed: {e}")
            return False
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
        traceback.print_exc()
        return False

def test_advanced_features():
    """Test advanced features like caching and async execution."""
    print("\nTesting advanced features...")
    
    try:
        from modelcard_generator.core.distributed_cache import LRUCache
        from modelcard_generator.core.rate_limiter import TokenBucket
        
        # Test caching
        cache = LRUCache(max_size=10)
        cache.put("test_key", "test_value")
        value = cache.get("test_key")
        assert value == "test_value"
        print("‚úÖ Caching works")
        
        # Test rate limiting
        bucket = TokenBucket(rate=10.0, capacity=10)
        import asyncio
        
        async def test_rate_limit():
            result = await bucket.acquire(1)
            return result
        
        result = asyncio.run(test_rate_limit())
        assert result is True
        print("‚úÖ Rate limiting works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Advanced features test failed: {e}")
        traceback.print_exc()
        return False

def run_all_tests():
    """Run all tests and report results."""
    print("üöÄ Running Model Card Generator Test Suite")
    print("=" * 50)
    
    tests = [
        ("Import Tests", test_imports),
        ("Basic Functionality", test_basic_functionality),
        ("Security Features", test_security_features),
        ("Performance Features", test_performance_features),
        ("Advanced Features", test_advanced_features)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\nüß™ {test_name}")
        print("-" * 30)
        
        try:
            if test_func():
                passed += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                failed += 1
                print(f"‚ùå {test_name} FAILED")
        except Exception as e:
            failed += 1
            print(f"‚ùå {test_name} FAILED with exception: {e}")
    
    print("\n" + "=" * 50)
    print(f"üìä Test Results: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("üéâ All tests passed! The system is working correctly.")
        return True
    else:
        print(f"‚ö†Ô∏è  {failed} test(s) failed. Check the output above for details.")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)