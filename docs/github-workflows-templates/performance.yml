# Performance Monitoring and Optimization Pipeline
# Implements continuous performance monitoring, benchmarking,
# and automated performance regression detection

name: Performance Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Daily performance benchmarks at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        type: choice
        options:
          - full
          - memory
          - cpu
          - io
          - network
          - custom
      performance_target:
        description: 'Performance target (baseline, optimized, stress)'
        required: false
        type: choice
        options:
          - baseline
          - optimized
          - stress
        default: 'baseline'

permissions:
  contents: write
  pull-requests: write
  issues: write

env:
  PYTHON_VERSION: '3.11'
  BENCHMARK_ITERATIONS: 100
  MEMORY_LIMIT_MB: 512
  CPU_LIMIT_CORES: 2

jobs:
  # Performance Benchmarking
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.11', '3.12']
        benchmark-suite: ['core', 'cli', 'templates', 'validation']
        
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          
      - name: Install dependencies and benchmarking tools
        run: |
          pip install -e ".[test,cli]"
          pip install pytest-benchmark memory-profiler psutil py-spy
          
      - name: Configure system for benchmarking
        run: |
          # Set CPU governor to performance mode for consistent results
          echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor || true
          
          # Disable CPU frequency scaling
          sudo cpupower frequency-set --governor performance || true
          
          # Set process priority
          sudo renice -10 $$
          
      - name: Run core performance benchmarks
        if: matrix.benchmark-suite == 'core'
        run: |
          pytest tests/performance/test_core_benchmarks.py \
            --benchmark-json=benchmark-core-${{ matrix.python-version }}.json \
            --benchmark-histogram=benchmark-core-${{ matrix.python-version }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_ITERATIONS }} \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=10
            
      - name: Run CLI performance benchmarks
        if: matrix.benchmark-suite == 'cli'
        run: |
          pytest tests/performance/test_cli_benchmarks.py \
            --benchmark-json=benchmark-cli-${{ matrix.python-version }}.json \
            --benchmark-histogram=benchmark-cli-${{ matrix.python-version }} \
            --benchmark-min-rounds=50 \
            --benchmark-warmup=on
            
      - name: Run template rendering benchmarks
        if: matrix.benchmark-suite == 'templates'
        run: |
          pytest tests/performance/test_template_benchmarks.py \
            --benchmark-json=benchmark-templates-${{ matrix.python-version }}.json \
            --benchmark-histogram=benchmark-templates-${{ matrix.python-version }} \
            --benchmark-min-rounds=200 \
            --benchmark-warmup=on
            
      - name: Run validation performance benchmarks
        if: matrix.benchmark-suite == 'validation'
        run: |
          pytest tests/performance/test_validation_benchmarks.py \
            --benchmark-json=benchmark-validation-${{ matrix.python-version }}.json \
            --benchmark-histogram=benchmark-validation-${{ matrix.python-version }} \
            --benchmark-min-rounds=100 \
            --benchmark-warmup=on
            
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmarks-${{ matrix.python-version }}-${{ matrix.benchmark-suite }}
          path: |
            benchmark-*.json
            benchmark-*.svg

  # Memory Profiling
  memory-profile:
    name: Memory Profiling
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install profiling tools
        run: |
          pip install -e ".[test]"
          pip install memory-profiler pympler tracemalloc psutil
          
      - name: Run memory profiling
        run: |
          # Profile memory usage of core functions
          python -m memory_profiler tests/performance/profile_memory.py > memory-profile.txt
          
          # Generate memory usage report
          python tests/performance/memory_analysis.py > memory-analysis.json
          
          # Check for memory leaks
          python tests/performance/memory_leak_detection.py > memory-leaks.txt
          
      - name: Analyze memory usage
        run: |
          python -c "
          import json
          import sys
          
          # Load memory analysis
          with open('memory-analysis.json') as f:
              data = json.load(f)
              
          # Check memory thresholds
          max_memory_mb = data.get('peak_memory_mb', 0)
          avg_memory_mb = data.get('average_memory_mb', 0)
          
          print(f'Peak memory usage: {max_memory_mb:.2f} MB')
          print(f'Average memory usage: {avg_memory_mb:.2f} MB')
          
          # Fail if memory usage exceeds limits
          if max_memory_mb > ${{ env.MEMORY_LIMIT_MB }}:
              print(f'ERROR: Memory usage exceeds limit of ${{ env.MEMORY_LIMIT_MB }} MB')
              sys.exit(1)
              
          # Check for potential memory leaks
          with open('memory-leaks.txt') as f:
              leaks = f.read().strip()
              if 'LEAK DETECTED' in leaks:
                  print('ERROR: Memory leaks detected')
                  print(leaks)
                  sys.exit(1)
          "
          
      - name: Upload memory profiling results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: memory-profiling
          path: |
            memory-profile.txt
            memory-analysis.json
            memory-leaks.txt

  # CPU Profiling
  cpu-profile:
    name: CPU Profiling
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install profiling tools
        run: |
          pip install -e ".[test]"
          pip install py-spy cProfile snakeviz
          
      - name: Run CPU profiling
        run: |
          # Profile with py-spy (sampling profiler)
          timeout 60s py-spy record -o cpu-profile.svg -d 60 -s -- python tests/performance/cpu_intensive_test.py || true
          
          # Profile with cProfile
          python -m cProfile -o cpu-profile.prof tests/performance/cpu_intensive_test.py
          
          # Generate CPU analysis report
          python tests/performance/cpu_analysis.py cpu-profile.prof > cpu-analysis.json
          
      - name: Analyze CPU usage
        run: |
          python -c "
          import json
          import sys
          
          try:
              with open('cpu-analysis.json') as f:
                  data = json.load(f)
                  
              # Check CPU performance metrics
              total_time = data.get('total_time', 0)
              cpu_intensive_functions = data.get('cpu_intensive_functions', [])
              
              print(f'Total CPU time: {total_time:.2f} seconds')
              print(f'CPU intensive functions: {len(cpu_intensive_functions)}')
              
              # Report top CPU consumers
              for func in cpu_intensive_functions[:5]:
                  print(f'  {func[\"function\"]}: {func[\"cumtime\"]:.2f}s ({func[\"percentage\"]:.1f}%)')
                  
              # Performance regression check
              if total_time > 30:  # 30 second threshold
                  print('WARNING: CPU usage higher than expected')
                  # Don't fail, just warn for now
                  
          except Exception as e:
              print(f'Error analyzing CPU profile: {e}')
          "
          
      - name: Upload CPU profiling results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: cpu-profiling
          path: |
            cpu-profile.svg
            cpu-profile.prof
            cpu-analysis.json

  # I/O Performance Testing
  io-performance:
    name: I/O Performance Testing
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          pip install -e ".[test]"
          pip install aiofiles
          
      - name: Run I/O performance tests
        run: |
          # Test file I/O performance
          python tests/performance/test_file_io.py > io-performance.json
          
          # Test template loading performance
          python tests/performance/test_template_loading.py >> io-performance.json
          
          # Test configuration loading performance
          python tests/performance/test_config_loading.py >> io-performance.json
          
      - name: Analyze I/O performance
        run: |
          python -c "
          import json
          import sys
          
          # Aggregate I/O performance metrics
          total_read_time = 0
          total_write_time = 0
          
          with open('io-performance.json') as f:
              for line in f:
                  if line.strip():
                      data = json.loads(line)
                      total_read_time += data.get('read_time', 0)
                      total_write_time += data.get('write_time', 0)
                      
          print(f'Total read time: {total_read_time:.2f}s')
          print(f'Total write time: {total_write_time:.2f}s')
          
          # Check I/O performance thresholds
          if total_read_time > 5.0:  # 5 second read threshold
              print('WARNING: File read performance slower than expected')
          if total_write_time > 2.0:  # 2 second write threshold
              print('WARNING: File write performance slower than expected')
          "
          
      - name: Upload I/O performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: io-performance
          path: io-performance.json

  # Performance Regression Detection
  regression-detection:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profile, cpu-profile, io-performance]
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Download all benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-results/
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install analysis tools
        run: |
          pip install numpy pandas matplotlib scipy
          
      - name: Compare performance with baseline
        run: |
          python scripts/performance_regression_analysis.py \
            --current benchmark-results/ \
            --baseline-ref origin/main \
            --output performance-comparison.json \
            --threshold 0.1  # 10% regression threshold
            
      - name: Generate performance report
        run: |
          python scripts/generate_performance_report.py \
            --comparison performance-comparison.json \
            --output performance-report.md
            
      - name: Comment performance results on PR
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 📊 Performance Analysis\n\n${report}`
            });
            
      - name: Check for performance regressions
        run: |
          python -c "
          import json
          import sys
          
          with open('performance-comparison.json') as f:
              data = json.load(f)
              
          regressions = data.get('regressions', [])
          
          if regressions:
              print(f'Performance regressions detected: {len(regressions)}')
              for regression in regressions:
                  print(f'  {regression[\"test\"]}: {regression[\"change\"]:+.1f}% slower')
              
              # Fail if critical regressions
              critical_regressions = [r for r in regressions if r['change'] > 20]
              if critical_regressions:
                  print('CRITICAL: Performance regressions > 20% detected')
                  sys.exit(1)
          else:
              print('No significant performance regressions detected')
          "
          
      - name: Upload performance analysis
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-analysis
          path: |
            performance-comparison.json
            performance-report.md

  # Performance Baseline Update
  update-baseline:
    name: Update Performance Baseline
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profile, cpu-profile, io-performance]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Download all benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-results/
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Update performance baseline
        run: |
          python scripts/update_performance_baseline.py \
            --results benchmark-results/ \
            --baseline-file performance-baseline.json
            
      - name: Commit performance baseline
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add performance-baseline.json
          git commit -m "chore: update performance baseline [skip ci]" || exit 0
          git push

  # Performance Monitoring Dashboard
  performance-dashboard:
    name: Update Performance Dashboard
    runs-on: ubuntu-latest
    needs: [benchmark, memory-profile, cpu-profile, io-performance]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all performance results
        uses: actions/download-artifact@v3
        with:
          path: performance-results/
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Generate performance dashboard
        run: |
          pip install plotly pandas numpy
          python scripts/generate_performance_dashboard.py \
            --results performance-results/ \
            --output docs/performance/
            
      - name: Deploy dashboard to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: docs/performance/
          destination_dir: performance
          
      - name: Create performance issue if needed
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Performance Issues Detected - ${new Date().toISOString().split('T')[0]}`,
              body: `## Performance Issues Detected
              
              The performance monitoring workflow has detected issues that require attention.
              
              **Failed Jobs:**
              - Benchmark: ${{ needs.benchmark.result }}
              - Memory Profile: ${{ needs.memory-profile.result }}  
              - CPU Profile: ${{ needs.cpu-profile.result }}
              - I/O Performance: ${{ needs.io-performance.result }}
              
              **Action Required:**
              1. Review performance artifacts in the failed workflow run
              2. Identify root causes of performance degradation
              3. Implement optimizations
              4. Re-run performance tests
              
              **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
              `,
              labels: ['performance', 'bug', 'needs-investigation']
            });